= Getting started with Nextflow

== Basic concepts

Nextflow is a workflow orchestration engine and a domain specific language (DSL)
that makes it easy to write data-intensive computational pipelines.

It is designed around the idea that the Linux platform is the _lingua franca_ of data science.
Linux provides many simple but powerful command-line and scripting tools that, when chained together,
facilitate complex data manipulations.

Nextflow extends this approach, adding the ability to define complex program interactions and a
high-level parallel computational environment based on the dataflow programming model. Nextflow's
core features are:

* workflow portability & reproducibility
* scalability of parallelization and deployment
* integration of existing tools, systems & industry standards

=== Processes and Channels

In practice, a Nextflow pipeline is made by joining together different processes.
Each process can be written in any scripting language that can be executed by the Linux platform
(Bash, Perl, Ruby, Python, etc.).

_Processes_ are executed independently and are isolated from each other, i.e. they do not share a common
(writable) state. The only way they can communicate is via asynchronous first-in, first-out (FIFO) queues, called
_Channels_ in Nextflow.

Any Process can define one or more Channels as input and output. The interaction between these Processes,
and ultimately the pipeline execution flow itself, is implicitly defined by these input and output declarations.

image::channel-process.png[]

=== Execution abstraction

While a Process defines _what_ command or script has to be executed, the executor determines
how that script is actually run in the target platform.

If not otherwise specified, processes are executed on the local computer. The local executor
is very useful for pipeline development and testing purposes, but for real world computational
pipelines, an HPC or cloud platform is often required.

In other words, Nextflow provides an abstraction between the pipeline's functional logic and
the underlying execution system (or runtime). Thus it is possible to write a pipeline once and to seamlessly
run it on your computer, a cluster, or the cloud, without modifying it, by simply defining
the target execution platform in the configuration file.

image::execution_abstraction.png[]

=== Scripting language

Nextflow implements a declarative domain specific language (DSL) that simplifies the writing 
of complex data analysis workflows as an extension of a general purpose programming language.

This approach makes Nextflow very flexible, because it allows in the same
computing environment the benefits of a concise DSL allowing the handling of
recurrent use cases with ease *and* the flexibility and power of a general purpose
programming language to handle corner cases, which may be difficult to implement using
a purely declarative approach.

In practical terms, Nextflow scripting is an extension of the https://groovy-lang.org/[Groovy programming language],
which in turn is a super-set of the Java programming language. Groovy can be considered as Python for Java in that
it simplifies the writing of code and is more approachable.



== Your first script

Here you will execute your first Nextflow script (hello.nf), but first we will go through the steps. 

=== Nextflow code (hello.nf)

[source,nextflow,linenums]
----
#!/usr/bin/env nextflow

params.greeting  = 'Hello world!'
greeting_ch = Channel.from(params.greeting)

process splitLetters {

    input:
    val x from greeting_ch

    output:
    file 'chunk_*' into letters_ch

    """
    printf '$x' | head -c 6 > chunk_aa
    printf '$x' | tail -c 6 > chunk_ab
    """
}

process convertToUpper {

    input:
    file y from letters_ch.flatten()

    output:
    stdout into result_ch

    """
    cat $y | tr '[a-z]' '[A-Z]' 
    """
}

result_ch.view{ it }
----

=== Line-by-line description

* *1*: The script begins with a shebang which declares Nextflow as the interepreter.

* *3*: Declares a Pipeline parameter `greeting` that is initialized with the value 'Hello world!'.

* *4*: Initalise a Channel `greeting_ch` which contains the value from `params.greeting`.

* *6*: Begin first Process defined as `splitLetters`.

* *8*: *Input* declaration for the splitLetters Process.

* *9*: Tell Process to expect an input value (`val`) *from* the channel 'greeting_ch', that we assign to the variable 'x'. 

* *11*: *Output* decalration for the splitLetters Process.

* *12*: Tell Process to expect output file/s (`file`) containing 'chunk_*' as output from the script and send the files to the channel 'letters_ch'. 

* *14*: Triple quote marks intiate the code block to execute in this Process.

* *15-16*: Code to execute, printing the input value x (called using the dollar operator [$]), and splitting the string into 2 chunks of 6 characters each ("Hello " and "world!") and saving to files: chunk_aa and chunk_ab.

* *17*: Triple quote marks the end of the code block.

* *18*: End of first Process block.

* *20*: Begin second Process defined as `convertToUpper`.

* *22*: *Input* declaration for the convertToUpper Process.

* *23*: Tell Process to expect input file/s (`file`; e.g. chunk_aa and chunk_ab) from the letter_ch, that we assign to the variable 'y'. 

TIP: The use of the operator `.flatten()` here is to split the two files into two separate items to be put through the next process (else they would treat them as a single element).

* *25*: *Output* declaration for the convertToUpper Process.

* *26*: Tell Process to expect output as standard output (stdout) and direct this `into` the `result_ch` channel.

* *28*: Triple quote marks intiate the code block to execute in this Process.

* *29*: Script to read files (cat) using the '$y' input variable, then pipe to uppercase conversion, outputting to standard output 

* *30*: Triple quote marks the end of the code block.

* *31*: End of first Process block.

* *33*: The final output (in the `result_ch`) is printed to screen using the `view` operator (appended onto the channel name). 

=== In DAG-like format

image::helloworlddiagram.png[]

Please now copy the 
following example into your favourite text editor and save it
to a file named `hello.nf` :

Execute the script by entering the following command in your terminal:

[source,cmd]
----
nextflow run hello.nf
----

It will output something similar to the text shown below:

[source,cmd]
----
N E X T F L O W  ~  version 20.10.0
Launching `hello.nf` [marvelous_plateau] - revision: 63f8ad7155
[warm up] executor > local
executor >  local (3)
[19/c2f873] process > splitWords   [100%] 1 of 1 ✔
[05/5ff9f6] process > convertToUpper [100%] 2 of 2 ✔
HELLO
WORLD!
----

You can see that the first process is executed once, and the second
twice. Finally the result string is printed.

It's worth noting that the process `convertToUpper` is executed in
parallel, so there's no guarantee that the instance processing the first
split (the chunk Hello) will be executed before the one
processing the second split (the chunk world!).

Thus, it is perfectly possible that you will get the final result
printed out in a different order:

[source,cmd]
....
WORLD!
HELLO
....

TIP: The hexadecimal numbers, like `22/7548fa`, identify the unique process
execution. These numbers are also the prefix of the directories where each
process is executed. You can inspect the files produced by them changing to the directory
`$PWD/work` and using these numbers to find the process-specific
execution path.

== Modify and resume

Nextflow keeps track of all the processes executed in your pipeline. If
you modify some parts of your script, only the processes that are
actually changed will be re-executed. The execution of the processes
that are not changed will be skipped and the cached result used instead.

This helps a lot when testing or modifying part of your pipeline without
having to re-execute it from scratch.

For the sake of this tutorial, modify the `convertToUpper` process in
the previous example, replacing the process script with the string
`rev $y`, so that the process looks like this:

[source,nextflow,linenums]
----
process convertToUpper {

    input:
    file y from letters.flatten()

    output:
    stdout into result

    """
    rev $y
    """
}
----

Then save the file with the same name, and execute it by adding the
`-resume` option to the command line:

[source,cmd]
----
nextflow run hello.nf -resume
----

It will print output similar to this:

[source,cmd]
----
N E X T F L O W  ~  version 20.10.0
Launching `hello.nf` [naughty_tuckerman] - revision: 22eaa07be4
[warm up] executor > local
executor >  local (2)
[19/c2f873] process > splitLetters     [100%] 1 of 1, cached: 1 ✔
[a7/a410d3] process > convertToUpper [100%] 2 of 2 ✔
olleH
!dlrow
----

You will see that the execution of the process `splitLetters` is
actually skipped (the process ID is the same), and its results are
retrieved from the cache. The second process is executed as expected,
printing the reversed strings.

TIP: The pipeline results are cached by default in the directory `$PWD/work`.
Depending on your script, this folder can take of lot of disk space.
If you are sure you won't resume your pipeline execution, clean this folder periodically.


== Pipeline parameters

Pipeline parameters are simply declared by prepending to a variable name
the prefix `params`, separated by dot character. Their value can be
specified on the command line by prefixing the parameter name with a
double dash character, i.e. `--paramName`

For the sake of this tutorial, you can try to execute the previous
example specifying a different input string parameter, as shown below:

[source,cmd]
----
nextflow run hello.nf --greeting 'Hola! mundo!'
----

The string specified on the command line will override the default value
of the parameter. The output will look like this:

[source,cmd]
----
N E X T F L O W  ~  version 20.10.0
Launching `hello.nf` [wise_stallman] - revision: 22eaa07be4
[warm up] executor > local
executor >  local (4)
[48/e8315b] process > splitLetters   [100%] 1 of 1 ✔
[01/840ca7] process > convertToUpper [100%] 3 of 3 ✔
 !aloh
!odnum
----
